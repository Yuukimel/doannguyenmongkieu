[
{
	"uri": "//localhost:1313/",
	"title": "Data Quality Management with Automated Validation on AWS",
	"tags": [],
	"description": "",
	"content": "Data Quality Management with Automated Validation on AWS Overview In large-scale, multi-source data environments, ensuring data quality is critical for accurate business decision-making.\nHowever, common issues such as:\nMissing data Incorrect formats Duplicate records ‚Ä¶ still frequently occur and can significantly impact analytical results.\nThe Data Quality Management with Automated Validation solution helps:\nAutomatically check data quality Detect and handle faulty data Reduce manual validation efforts Enable real-time monitoring with AWS services üì¶ AWS Services Used AWS Glue\nA serverless ETL (Extract, Transform, Load) service used to scan, classify data, and define Data Quality Rules via the Data Quality Definition Language (DQDL).\nAWS Lambda\nA serverless compute service used to automatically trigger data validation, process results, and send alerts.\nAmazon S3\nAn object storage service used to store source data, validation results, rule configuration files, and reports.\nMain Content Create an S3 bucket to upload data files Grant IAM permissions Set up AWS Glue Delete resources "
},
{
	"uri": "//localhost:1313/1-s3bucket/1.1-setup-s3-bucket/",
	"title": "S3 Bucket in Data Quality Management Workshop",
	"tags": [],
	"description": "",
	"content": "Purpose of Use Store raw data before running profiling or validation. Store processed/validated data after cleaning and checks. Store data quality check results from Glue Data Quality (reports, metrics). Act as a staging area for scripts or configuration files. Cost Considerations S3 bucket incurs no cost when empty or when requests are minimal. Delete temporary files or unnecessary test data to avoid charges. You can keep an empty bucket for reuse in later steps. When Stopping the Service You can delete the data inside the bucket to save storage costs. To restart the workshop, simply re-upload the required data or scripts into the bucket. "
},
{
	"uri": "//localhost:1313/2-mfa-setup-for-aws-user-root/1-iam-role-for-aws-glue/",
	"title": "Set up IAM Role for AWS Glue",
	"tags": [],
	"description": "",
	"content": "Granting Permissions for AWS Glue Objective Create an IAM Role for Glue that allows:\nAccess to S3 (read/write data) Use of Glue Crawlers, Jobs, and Data Quality Ruleset Write logs to CloudWatch Logs (for monitoring job execution) Step 1: Create the Role Go to IAM ‚Üí Roles ‚Üí Create Role. In Select trusted entity, choose: AWS service Use case: Glue (type ‚ÄúGlue‚Äù into the search box and select it). Step 2: Attach Permission Policies Search and check the following policies:\nPolicy name Purpose AmazonS3FullAccess (or AmazonS3ReadOnlyAccess + specific bucket write access) Allows Glue to read/write files to S3 AWSGlueServiceRole Basic permissions for Glue (Crawler, Job, Catalog‚Ä¶) CloudWatchLogsFullAccess (or specific permissions: logs:CreateLogGroup, logs:PutLogEvents) Allows Glue to write job execution logs Step 3: Name and Create the Role Role name: GlueDQWorkshopRole\nDescription: \u0026ldquo;Role for Glue Job to read data from S3, write logs, and execute DQ rules\u0026rdquo;\nClick Next.\nNote: Select the database set up in AWS Glue to proceed with adding the Role.\n"
},
{
	"uri": "//localhost:1313/1-s3bucket/",
	"title": "Set up S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Content:\nInitialize Data Sample Data Upload File to S3 Initialize Data In this step, we will create a simulated dataset to test Data Quality Rules in AWS Glue.\nThe dataset will contain order information (order_id, customer_id, order_date, order_status, total_amount) with some intentional data errors for testing purposes.\nHow to create the file:\nOpen a text editor (Notepad, VS Code, ‚Ä¶) Copy the content from the Sample Data section Save the file as orders.csv Make sure the file is encoded in UTF-8 and columns are separated by commas (,) Sample Data order_id,customer_id,order_date,order_status,total_amount 1001,C001,2023-01-10,completed,120.50 1002,,2023-01-12,completed,89.00 1003,C003,2023/01/13,pending,99.99 1004,C004,2023-01-14,completed,75.25 1002,C002,2023-01-12,completed,89.00 1005,C005,2023-01-15,canceledd,105.00 1006,C006,2023-01-16,pending,error 1007,C007,2023-13-01,completed,110.00 1008,C008,,completed,130.00 1009,C009,2023-01-18,in_transit,95.00 1010,C010,2023-01-19,completed, 1011,C011,19-01-2023,completed,90.50 1012,C011,2023-01-20,completed,90.50 1013,,2023-01-21,cancelled,abc "
},
{
	"uri": "//localhost:1313/1-s3bucket/1.2-setup-data/",
	"title": "Sample Data Description",
	"tags": [],
	"description": "",
	"content": "The sample dataset used in the Data Quality Management with AWS Glue Workshop contains customer and transaction information, designed for testing Data Quality Rules.\nFormat File format: CSV Encoding: UTF-8 Delimiter: Comma (,) Main Columns Column Name Data Type Description customer_id String Unique identifier for each customer name String Full name of the customer email String Email address phone String Phone number dob Date Date of birth (YYYY-MM-DD format) join_date Date Date the customer joined the system status String Customer status (active, inactive) Simulated Data Quality Issues The sample dataset is intentionally seeded with common issues to demonstrate Data Quality Rule applications:\nNull values in required fields (customer_id, email). Invalid date format in the dob column. Duplicate customer_id entries. Invalid email format (missing @ symbol or domain). Incorrect phone number format. Purpose Verify AWS Glue Data Quality‚Äôs ability to detect data errors. Practice writing and applying a Data Quality Rule Set. Evaluate data quality before and after rule application. "
},
{
	"uri": "//localhost:1313/2-mfa-setup-for-aws-user-root/2-setup-awsglue-crawlers/",
	"title": "Set up AWS Glue Crawlers",
	"tags": [],
	"description": "",
	"content": "Create Glue Crawler to Detect Schema Go to AWS Glue Console ‚Üí Crawlers ‚Üí Create crawler.\nSet Crawler name: orders-crawler. Data source: S3 ‚Üí Select s3://dq-workshop-data/input/. IAM role: Select the role you created earlier (e.g., GlueDQWorkshopRole). Database: Create a new one named dq_db (or choose an existing one). Create a new database\nTable name: data.\nRun the crawler ‚Üí Verify if the data table appears in Glue Data Catalog.\n"
},
{
	"uri": "//localhost:1313/2-mfa-setup-for-aws-user-root/",
	"title": "Set up IAM Role, AWS Glue",
	"tags": [],
	"description": "",
	"content": "Introduction to IAM Root User The IAM Root User is the highest-level administrative account in AWS, automatically created when you first sign up for an AWS account.\nThis account has unrestricted access to all AWS services and resources.\nKey Characteristics: Full access to all services, including the ability to modify billing settings and delete the account. Cannot be deleted or have its permissions restricted. Only one Root User exists for each AWS account. Security Recommendations: Enable Multi-Factor Authentication (MFA) for the Root User. Avoid using the Root User for daily operations ‚Äî instead, create IAM users/roles with the necessary permissions. Store Root User credentials in a secure location and only use them when absolutely necessary (e.g., changing billing settings, account recovery). Enable CloudTrail logging to monitor any activity related to the Root User. üìå Note: If the Root User credentials are compromised, you could lose complete control over your AWS account.\nIntroduction to AWS Glue AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services.\nIt helps you prepare and load data for analytics, machine learning, and application development without having to manually manage servers or infrastructure.\nKey Features Serverless: No infrastructure to manage. AWS Glue automatically provisions the environment. Data Catalog: Centralized metadata repository to store table definitions, schema, and job metadata. Built-in ETL Engine: Automatically generates Python or Scala code for ETL jobs. Job Scheduling: Schedule ETL jobs or trigger them based on events. Integration with AWS Services: Works seamlessly with Amazon S3, Redshift, RDS, Athena, and more. AWS Glue Architecture Overview Data Catalog Stores metadata about your datasets. Crawlers Automatically scan data sources and update the Data Catalog. ETL Jobs Transform and move data using Apache Spark under the hood. Triggers \u0026amp; Workflow Automate job execution and dependencies. Common Use Cases Data preparation for analytics and machine learning. Data integration from multiple sources. Metadata management across different AWS data services. Automated validation and transformation in Data Quality Management workflows. "
},
{
	"uri": "//localhost:1313/3-running-data-profiling/",
	"title": "Running Data Profiling on a Table",
	"tags": [],
	"description": "",
	"content": "Step 1: Open the Table from Glue Data Catalog Go to AWS Console ‚Üí Search for AWS Glue. Select the Tables tab. Choose the database workshop-db (or the name you assigned). Click on the table created by the crawler, for example: orders_data. Step 2: Run Data Profiling On the table details page, select the Data quality tab. Click Run data profiling. If no ruleset exists yet, proceed to create a Data Quality Ruleset. Create a Data Quality Ruleset\nIn the Data quality tab, click Create data quality rule. In the Create data quality ruleset section, select: Ruleset name: dq_rules_orders Table: workshop_data_pj Create rules using: Glue Data Quality DSL Click Next Proceed to add rules After setting up the rules (DQDL) for automated checks: Click Next ‚Üí Create ruleset In the Data quality list, you will see dq_rules_orders Click the Run button to execute data quality checks Step 3: Review Data Quality Ruleset Results Open AWS Glue Console. On the left sidebar, select Data quality. You will see the list of created rulesets. If not run yet: click Run on the ruleset you created to perform data checks. If already run: click on the ruleset name to view results. After the ruleset runs, the results page displays:\nRuleset name and status: PASS or FAIL Metrics: number of error rows, pass/fail percentages Data source: table name scanned from S3 by the crawler Last run time: the most recent run timestamp You can rerun the ruleset directly from the interface üìå AWS Glue performs profiling as follows:\nCounts the number of records. Calculates the NULL ratio per column. Infers data types (string, int, date, etc.). Counts unique values. Determines min/max and average values (if numeric). "
},
{
	"uri": "//localhost:1313/2-mfa-setup-for-aws-user-root/3-create-database/",
	"title": "Create a New Database",
	"tags": [],
	"description": "",
	"content": "Creating a New Database in AWS Glue Open the Glue Console\nGo to AWS Console ‚Üí Search for \u0026ldquo;AWS Glue\u0026rdquo; ‚Üí Select Data Catalog ‚Üí Databases.\nClick Create database.\nEnter the details:\nDatabase name: for example, orders\nLocation (optional): You can specify an S3 path to store the metadata. Description (optional): Describe the database purpose, for example: Database for order data in the DQ workshop. Click Create database.\n"
},
{
	"uri": "//localhost:1313/4-deleting-resources/",
	"title": "Deleting Resources",
	"tags": [],
	"description": "",
	"content": "Contents:\nPausing Amazon S3 Buckets AWS Glue Crawler Glue Data Quality Rule Sets (DQDL) After finishing your work, you should immediately stop any services that may incur costs, even if you have not used them much ‚Äî because AWS charges based on storage time and running services.\nPausing Amazon S3 Buckets Delete files (objects) in the S3 bucket Go to the S3 service.\nSelect the bucket you want to delete.\nOpen the Objects tab (or Files).\nSelect the files or folders to delete (you can select all).\nClick the Delete button.\nDelete the S3 bucket After the bucket is empty:\nIn the bucket list page, find the bucket you want to delete.\nTick the checkbox next to the bucket.\nClick the Delete button.\nEnter the bucket name as requested to confirm.\nClick Confirm to complete the bucket deletion.\nAWS Glue Crawler Go to AWS Glue \u0026gt; Crawlers\nSelect the crawler you created ‚Üí click Delete\nGlue Data Quality Rule Sets (DQDL) Stop running or delete if you do not need to keep them\nGo to AWS Glue \u0026gt; Data quality \u0026gt; Rule sets ‚Üí Delete if unused\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]